{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "stainless-windows",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version: 2.3.3\n",
      "Using repo_root: c:\\Users\\jonas\\Desktop\\repos\\pixlball\n",
      "Signature check: (events_df, nn_layers_df, target_cols=['nn_target'], id_col='id', context_cols=False, temporal_context=True, keep_context_ids=False)\n"
     ]
    }
   ],
   "source": [
    "# 1. Load standard libraries FIRST\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "import importlib\n",
    "import inspect\n",
    "import os\n",
    "\n",
    "# 2. Verify standard libraries are healthy\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "\n",
    "# --- 1. Path Setup ---\n",
    "# Try to locate the repository root by searching upward for a 'src' directory (or .git)\n",
    "def find_repo_root(start_path=None, marker_dirs=('src', '.git')):\n",
    "    p = os.path.abspath(start_path or os.getcwd())\n",
    "    while True:\n",
    "        if any(os.path.isdir(os.path.join(p, m)) for m in marker_dirs):\n",
    "            return p\n",
    "        parent = os.path.dirname(p)\n",
    "        if parent == p:\n",
    "            return None\n",
    "        p = parent\n",
    "\n",
    "repo_root = find_repo_root()\n",
    "# Fallback to previous hardcoded path working on nuvolos\n",
    "if repo_root is None:\n",
    "    repo_root = \"/files/pixlball\"\n",
    "\n",
    "if repo_root not in sys.path:\n",
    "    sys.path.insert(0, repo_root)\n",
    "print(f\"Using repo_root: {repo_root}\")\n",
    "\n",
    "import src.data as data\n",
    "import src.model as model\n",
    "import src.train as train\n",
    "import src.config as config\n",
    "import src.dataset as dataset\n",
    "import src.evaluate as evaluate\n",
    "import src.utils as utils\n",
    "\n",
    "from src.config import DEVICE \n",
    "\n",
    "\n",
    "# 4. Force a clean reload of your specific logic\n",
    "importlib.reload(data)\n",
    "importlib.reload(train)\n",
    "\n",
    "# 5. THE SMOKE TEST\n",
    "print(\"Signature check:\", inspect.signature(data.prepare_nn_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "atomic-blink",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_events = pd.read_parquet(os.path.join(repo_root, \"data\", \"events_data.parquet\"), engine=\"fastparquet\")\n",
    "data_360 = pd.read_parquet(os.path.join(repo_root, \"data\", \"sb360_data.parquet\"), engine=\"fastparquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "tropical-third",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2462 events.\n",
      "counts of each outcome nn_target\n",
      "Keep Possession    70920\n",
      "Lose Possession    27465\n",
      "Shot                4764\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_with_targets = data.event_data_loader(data_events)\n",
    "df_with_targets = data.add_ball_trajectory_features(df_with_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catholic-collect",
   "metadata": {},
   "source": [
    "# Prepare 360 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "boolean-paintball",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_360 = data.assign_grid_cells(data_360)\n",
    "nn_final = data.aggregate_nn_layers_vectorized(df_360)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-swedish",
   "metadata": {},
   "source": [
    "# Finalize NN Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "chubby-romania",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_dataset = data.prepare_nn_dataset(df_with_targets, nn_final, target_cols=['nn_target', 'goal_flag'], context_cols = True, keep_context_ids = True ) # adjust cols depending on model\n",
    "nn_dataset = data.add_context_cols(nn_dataset)\n",
    "nn_dataset = data.add_target_as_int(nn_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incredible-lucas",
   "metadata": {},
   "source": [
    "# Neural Network final Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb93c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# --- Usage ---\n",
    "# nn_dataset = add_ball_coordinates(nn_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee62f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_dataset, vector_names = data.add_ball_coordinates(nn_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "widespread-royal",
   "metadata": {},
   "source": [
    "# The Goal Multi Task CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "residential-ethics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goal Positive Weight (0/1 ratio): 5.00\n"
     ]
    }
   ],
   "source": [
    "layer_columns = [\"ball_layer\", \"teammates_layer\", \"opponents_layer\"]\n",
    "class_weights_event, goal_pos_weight = utils.get_multitask_loss_weights(nn_dataset, DEVICE)\n",
    "\n",
    "print(f\"Goal Positive Weight (0/1 ratio): {goal_pos_weight.item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-variety",
   "metadata": {},
   "source": [
    "# Preparing the Context CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "pharmaceutical-appendix",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 72117\n",
      "Total validation samples: 18030\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# 1. Define your split parameters\n",
    "VALIDATION_SIZE = 0.20\n",
    "RANDOM_SEED = 42\n",
    "layer_columns = [\"ball_layer\", \"teammates_layer\", \"opponents_layer\"]\n",
    "\n",
    "# 2. Split the entire DataFrame first\n",
    "# This keeps features, event targets, and goal flags bundled together\n",
    "train_df, val_df = train_test_split(\n",
    "    nn_dataset, \n",
    "    test_size=VALIDATION_SIZE, \n",
    "    random_state=RANDOM_SEED, \n",
    "    stratify=nn_dataset['nn_target_int']\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 3. Extract the arrays and Instantiate the Datasets (FIXED)\n",
    "# -------------------------------------------------------------\n",
    "context_features = ['under_pressure', 'counterpress', 'dribble_nutmeg']\n",
    "\n",
    "# Training Dataset extraction - Pass only the values in the correct order\n",
    "train_dataset_temporal_context = dataset.ContextBallVectorPitchDatasetMultiTask(\n",
    "    train_df[layer_columns],             # This maps to the 1st argument (features)\n",
    "    train_df['nn_target_int'].values,    # This maps to the 2nd argument (events)\n",
    "    train_df['goal_flag'].values,\n",
    "    train_df[vector_names]        # This maps to the 3rd argument (goals)\n",
    ")\n",
    "\n",
    "# Validation Dataset extraction\n",
    "validation_dataset_temporal_context = dataset.ContextBallVectorPitchDatasetMultiTask(\n",
    "    val_df[layer_columns], \n",
    "    val_df['nn_target_int'].values, \n",
    "    val_df['goal_flag'].values,\n",
    "    val_df[vector_names]  \n",
    ")\n",
    "\n",
    "print(f\"Total training samples: {len(train_dataset_temporal_context)}\")\n",
    "print(f\"Total validation samples: {len(validation_dataset_temporal_context)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "promotional-providence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for Contextual CNN Baseline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Context CNN Epoch 1: 100%|██████████| 2254/2254 [00:46<00:00, 48.76it/s, ev_loss=0.3339, loss=1.1469, sh_loss=0.5420] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Contextual CNN Training complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming event_class_weights and goal_pos_weight are defined from previous cells\n",
    "NUM_CONTEXT_FEATURES = 8 \n",
    "\n",
    "print(\"Starting training for Contextual CNN Baseline...\")\n",
    "\n",
    "# Modified the Function in Loss to take correct loss function -> needs to be changed for baseline model again\n",
    "\n",
    "context_baseline_model = train.train_model_context_threat(\n",
    "    dataset=train_dataset_temporal_context, \n",
    "    event_class_weights=class_weights_event, # Use your calculated weights\n",
    "    goal_pos_weight=goal_pos_weight,         # Use your calculated pos_weight\n",
    "    num_context_features=NUM_CONTEXT_FEATURES\n",
    ")\n",
    "\n",
    "print(\"\\nContextual CNN Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fabulous-warning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Contextual CNN Model...\n",
      "\n",
      "--- Event Outcome Metrics ---\n",
      "Event Accuracy: 0.22584581253466446\n",
      "Event Balanced Accuracy: 0.5220945566421485\n",
      "Event Confusion Matrix:\n",
      " [[   0 9647 2841]\n",
      " [   0 3316 1341]\n",
      " [   0  129  756]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00     12488\n",
      "           1       0.25      0.71      0.37      4657\n",
      "           2       0.15      0.85      0.26       885\n",
      "\n",
      "    accuracy                           0.23     18030\n",
      "   macro avg       0.14      0.52      0.21     18030\n",
      "weighted avg       0.07      0.23      0.11     18030\n",
      "\n",
      "\n",
      "--- Goal Prediction (xG) Metrics ---\n",
      "Goal Accuracy: 0.5932203389830508\n",
      "Goal Balanced Accuracy: 0.5402127562015682\n",
      "Goal AUC-ROC Score: 0.5774909441056445\n",
      "Goal Confusion Matrix:\n",
      " [[472 300]\n",
      " [ 60  53]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.61      0.72       772\n",
      "         1.0       0.15      0.47      0.23       113\n",
      "\n",
      "    accuracy                           0.59       885\n",
      "   macro avg       0.52      0.54      0.48       885\n",
      "weighted avg       0.79      0.59      0.66       885\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonas\\miniconda3\\envs\\standard\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\jonas\\miniconda3\\envs\\standard\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\jonas\\miniconda3\\envs\\standard\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "# Assuming evaluate_model_context is imported and available\n",
    "\n",
    "print(\"\\nEvaluating Contextual CNN Model...\")\n",
    "\n",
    "metrics = evaluate.evaluate_model_context_threat(\n",
    "    model=context_baseline_model, \n",
    "    dataset=validation_dataset_temporal_context # Evaluate on the contextual dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "competent-bradley",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Keep) | P(Lose) | P(Shot)\n",
      "-------------------------------\n",
      "[[0.35842028 0.5013404  0.1402393 ]\n",
      " [0.28389135 0.5476516  0.16845714]\n",
      " [0.13027388 0.33651263 0.53321356]\n",
      " [0.35667905 0.50527805 0.1380429 ]\n",
      " [0.15343137 0.23481005 0.6117586 ]]\n",
      "\n",
      "Average Predicted P(Shot) across all events: 0.3139\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Assuming metrics contains the result from evaluate_model_context_threat\n",
    "\n",
    "event_probs = metrics['event_probs']\n",
    "\n",
    "print(\"P(Keep) | P(Lose) | P(Shot)\")\n",
    "print(\"-------------------------------\")\n",
    "print(event_probs[:5])\n",
    "\n",
    "# You can look at the average predicted probability for the Shot class across all events:\n",
    "avg_p_shot = np.mean(event_probs[:, 2])\n",
    "print(f\"\\nAverage Predicted P(Shot) across all events: {avg_p_shot:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cutting-coast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Goal Prediction Probabilities (xG) Analysis ---\n",
      "Number of Shots Evaluated: 885\n",
      "\n",
      "Total Predicted xG: 422.83\n",
      "Total True Goals Scored: 113.00\n",
      "Average Predicted xG per Shot: 0.4778\n",
      "\n",
      "-- Calibration Check --\n",
      "Average xG for True Goals (should be high): 0.5005\n",
      "Average xG for Missed Shots (should be low): 0.4745\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Assuming metrics contains the result from evaluate_model_context_threat\n",
    "\n",
    "print(\"--- Goal Prediction Probabilities (xG) Analysis ---\")\n",
    "\n",
    "goal_probs = metrics['goal_probs']\n",
    "goal_labels = metrics['goal_labels'] # Actual outcome (0=No Goal, 1=Goal)\n",
    "\n",
    "print(f\"Number of Shots Evaluated: {len(goal_probs)}\")\n",
    "\n",
    "# 1. Total xG vs. Actual Goals\n",
    "total_predicted_xg = np.sum(goal_probs)\n",
    "total_true_goals = np.sum(goal_labels)\n",
    "avg_xg_per_shot = np.mean(goal_probs)\n",
    "\n",
    "print(f\"\\nTotal Predicted xG: {total_predicted_xg:.2f}\")\n",
    "print(f\"Total True Goals Scored: {total_true_goals:.2f}\")\n",
    "print(f\"Average Predicted xG per Shot: {avg_xg_per_shot:.4f}\")\n",
    "\n",
    "# 2. Calibration Check (Optional but helpful)\n",
    "# Compare the average predicted xG for shots that were goals vs. shots that were misses.\n",
    "\n",
    "# Create a DataFrame for easy slicing\n",
    "xg_df = pd.DataFrame({'xg': goal_probs, 'goal': goal_labels})\n",
    "\n",
    "avg_xg_goal = xg_df[xg_df['goal'] == 1]['xg'].mean()\n",
    "avg_xg_miss = xg_df[xg_df['goal'] == 0]['xg'].mean()\n",
    "\n",
    "print(\"\\n-- Calibration Check --\")\n",
    "print(f\"Average xG for True Goals (should be high): {avg_xg_goal:.4f}\")\n",
    "print(f\"Average xG for Missed Shots (should be low): {avg_xg_miss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
