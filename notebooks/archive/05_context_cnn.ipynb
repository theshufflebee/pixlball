{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "limiting-python",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import importlib\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- 1. Path Setup ---\n",
    "# Absolute path to repo root (adjust if necessary)\n",
    "repo_root = \"/files/pixlball\"\n",
    "if repo_root not in sys.path:\n",
    "    sys.path.insert(0, repo_root) \n",
    "\n",
    "# --- 2. Project Module Imports ---\n",
    "# Import all project modules using clean names\n",
    "import src.config as config\n",
    "import src.dataset as dataset\n",
    "import src.train as train\n",
    "import src.evaluate as evaluate\n",
    "import src.data as data\n",
    "import src.losses as losses\n",
    "import src.model as model\n",
    "import src.utils as utils\n",
    "\n",
    "# --- 3. Module Reloading (CRITICAL for Notebook Development) ---\n",
    "# Reload dependencies in order: Config/Utils -> Data/Losses/Model -> Train/Dataset/Evaluate\n",
    "importlib.reload(config)\n",
    "importlib.reload(utils)\n",
    "importlib.reload(data)\n",
    "importlib.reload(model)\n",
    "importlib.reload(losses) \n",
    "importlib.reload(dataset)\n",
    "importlib.reload(train)\n",
    "importlib.reload(evaluate)\n",
    "\n",
    "# --- 4. Direct Imports (For clean code in subsequent cells) ---\n",
    "# Import essential classes and functions needed for the pipeline steps\n",
    "\n",
    "# Configuration\n",
    "from src.config import DEVICE \n",
    "\n",
    "# Data/Dataset Classes\n",
    "from src.dataset import PitchDatasetMultiTask, TemporalPitchDataset, ContextPitchDatasetMultiTask, FusionPitchDataset\n",
    "\n",
    "# Training Functions\n",
    "from src.train import train_model_base, train_model_lstm, train_model_context, train_model_lstm_fused\n",
    "\n",
    "# Evaluation/Helpers\n",
    "from src.evaluate import evaluate_model_base, evaluate_model_lstm, evaluate_model_context, evaluate_model_lstm_fused\n",
    "from src.losses import get_model_criteria\n",
    "from src.model import TinyCNN_MultiTask, HybridCNN_LSTM, TinyCNN_LSTM_Fused\n",
    "from src.utils import get_sequence_lengths\n",
    "\n",
    "# --- Final Check ---\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-bearing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "irish-practitioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_events = pd.read_parquet(os.path.join(repo_root, \"data\", \"events_data.parquet\"), engine=\"fastparquet\")\n",
    "data_360 = pd.read_parquet(os.path.join(repo_root, \"data\", \"sb360_data.parquet\"), engine=\"fastparquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "round-thinking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1278 events.\n"
     ]
    }
   ],
   "source": [
    "admin_events = [\n",
    "        'Starting XI', 'Half Start', 'Half End', 'Player On', 'Player Off',\n",
    "        'Substitution', 'Tactical Shift', 'Referee Ball-Drop', 'Injury Stoppage',\n",
    "        'Bad Behaviour', 'Shield'\n",
    "    ]\n",
    "\n",
    "cleaned_df = data.drop_events(data_events, rows_to_drop=admin_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "numerous-apparatus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts of each outcome nn_target\n",
      "Keep Possession    71251\n",
      "Lose Possession    28252\n",
      "Shot                4830\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Example usage\n",
    "# -----------------------------\n",
    "columns_to_drop = ['clearance_body_part',\n",
    "                   'clearance_head',\n",
    "                   'clearance_left_foot',\n",
    "                   'clearance_other',\n",
    "                   'clearance_right_foot',\n",
    "                   'shot_technique',\n",
    "                   'substitution_replacement_id',\n",
    "                   'substitution_replacement',\n",
    "                   'substitution_outcome',\n",
    "                   'shot_saved_off_target',\n",
    "                   'pass_miscommunication',\n",
    "                   'goalkeeper_shot_saved_off_target',\n",
    "                   'goalkeeper_punched_out',\n",
    "                   'shot_first_time',\n",
    "                   'shot_first_time',\n",
    "                   'shot_body_part',\n",
    "                   'related_events',\n",
    "                   'pass_shot_assist', \n",
    "                   'pass_straight', \n",
    "                   'pass_switch', \n",
    "                   'pass_technique', \n",
    "                   'pass_through_ball',\n",
    "                   'goalkeeper_body_part',\n",
    "                   'goalkeeper_end_location', \n",
    "                   'goalkeeper_outcome', \n",
    "                   'goalkeeper_position', \n",
    "                   'goalkeeper_technique', \n",
    "                   'goalkeeper_type', \n",
    "                   'goalkeeper_penalty_saved_to_post', \n",
    "                   'goalkeeper_shot_saved_to_post', \n",
    "                   'goalkeeper_lost_out', \n",
    "                   'goalkeeper_Clear', \n",
    "                   'goalkeeper_In Play Safe',\n",
    "                   'shot_key_pass_id',\n",
    "                   'shot_one_on_one',\n",
    "                   'shot_end_location',\n",
    "                   'shot_type',\n",
    "                   'pass_angle',\n",
    "                   'pass_body_part',\n",
    "                   'pass_type',\n",
    "                   'pass_length',\n",
    "                   'pass_outswinging',\n",
    "                   'pass_inswinging',\n",
    "                   'pass_cross', \n",
    "                   'pass_cut_back', \n",
    "                   'pass_deflected', \n",
    "                   'pass_goal_assist', \n",
    "                   'pass_recipient', \n",
    "                   'pass_recipient_id', \n",
    "                   'pass_assisted_shot_id', \n",
    "                   'pass_no_touch', \n",
    "                   'pass_end_location', \n",
    "                   'pass_aerial_won',\n",
    "                   'pass_height',\n",
    "                   'substitution_outcome_id',\n",
    "                   'tactics',\n",
    "                   'block_deflection',\n",
    "                   'dribble_no_touch',\n",
    "                   'shot_open_goal', \n",
    "                   'shot_saved_to_post',\n",
    "                   'shot_redirect', \n",
    "                   'shot_follows_dribble',\n",
    "                   'period',\n",
    "                   'injury_stoppage_in_chanin',\n",
    "                   'block_save_block',\n",
    "                   'ball recovery_offensive',\n",
    "\n",
    "\n",
    "                   ]\n",
    "cleaned_df = data.drop_columns(cleaned_df, columns_to_drop)\n",
    "\n",
    "# add lookahead outcome\n",
    "df_with_targets = data.assign_lookahead_outcomes(cleaned_df, lookahead=6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-celebrity",
   "metadata": {},
   "source": [
    "# Prepare 360 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "apparent-weekend",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_360 = data.assign_grid_cells(data_360)\n",
    "nn_final = data.aggregate_nn_layers_vectorized(df_360)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banned-illness",
   "metadata": {},
   "source": [
    "# Finalize NN Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "advanced-progressive",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_dataset = data.prepare_nn_dataset(df_with_targets, nn_final, target_cols=['nn_target', 'goal_flag'], context_cols = True, keep_context_ids = True ) # adjust cols depending on model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "photographic-parameter",
   "metadata": {},
   "source": [
    "# Neural Network final Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "electronic-distinction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         nn_target  nn_target_int\n",
      "0  Keep Possession              0\n",
      "1  Keep Possession              0\n",
      "2  Keep Possession              0\n",
      "3  Keep Possession              0\n",
      "4  Keep Possession              0\n"
     ]
    }
   ],
   "source": [
    "context_cols = [\n",
    "    'under_pressure', \n",
    "    'counterpress', \n",
    "    'dribble_nutmeg'\n",
    "]\n",
    "\n",
    "# Impute NaN values with 0.0 (float)\n",
    "# This assumes NaN means the event was NOT under pressure, NOT a counterpress, etc.\n",
    "nn_dataset[context_cols] = nn_dataset[context_cols].fillna(0.0)\n",
    "\n",
    "\n",
    "target_map = {\"Keep Possession\": 0, \"Lose Possession\": 1, \"Shot\": 2}\n",
    "\n",
    "# Apply mapping\n",
    "nn_dataset['nn_target_int'] = nn_dataset['nn_target'].map(target_map)\n",
    "\n",
    "# Check\n",
    "print(nn_dataset[['nn_target', 'nn_target_int']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reverse-breakfast",
   "metadata": {},
   "source": [
    "# The Goal Multi Task CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "together-knock",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goal Positive Weight (0/1 ratio): 168.51\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------\n",
    "# 1. Define input columns & targets\n",
    "# ------------------------------------\n",
    "# This assumes nn_dataset is already loaded and processed in previous cells.\n",
    "layer_columns = [\"ball_layer\", \"teammates_layer\", \"opponents_layer\"]\n",
    "\n",
    "# Ensure labels are in the correct format\n",
    "event_targets = nn_dataset['nn_target_int'].values   # 0=keep, 1=lose, 2=shot (int)\n",
    "# CRITICAL: Goal flags must be float for BCEWithLogitsLoss\n",
    "goal_flags = nn_dataset['goal_flag'].values.astype(np.float32) \n",
    "\n",
    "# ------------------------------------\n",
    "# 3. Compute class weights and positive weight\n",
    "# ------------------------------------\n",
    "\n",
    "# A. Event Weights (Multi-Class) - For CrossEntropyLoss\n",
    "event_counts = Counter(event_targets)\n",
    "total_events = len(event_targets)\n",
    "\n",
    "# Using inverse frequency: total / count\n",
    "class_weights_event = torch.tensor(\n",
    "    [total_events / event_counts.get(c, 1) for c in range(len(event_counts))],\n",
    "    dtype=torch.float32\n",
    ").to(DEVICE)\n",
    "\n",
    "# B. Goal Positive Weight (Binary) - For BCEWithLogitsLoss\n",
    "goal_counts = Counter(goal_flags)\n",
    "# CRITICAL: pos_weight = (Number of Negative Samples) / (Number of Positive Samples)\n",
    "# Here: pos_weight = (Number of No Goals) / (Number of Goals)\n",
    "goal_pos_weight = torch.tensor(\n",
    "    goal_counts.get(0.0, 1) / goal_counts.get(1.0, 1),\n",
    "    dtype=torch.float32\n",
    ").to(DEVICE)\n",
    "\n",
    "print(f\"Goal Positive Weight (0/1 ratio): {goal_pos_weight.item():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-operations",
   "metadata": {},
   "source": [
    "# Preparing the Context CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "veterinary-oliver",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Contextual Features being used: 3\n",
      "Contextual Features Head:\n",
      "   under_pressure  counterpress  dribble_nutmeg\n",
      "0             0.0           0.0             0.0\n",
      "1             0.0           0.0             0.0\n",
      "2             1.0           0.0             0.0\n",
      "3             1.0           0.0             0.0\n",
      "4             0.0           0.0             0.0\n",
      "Contextual Dataset Size: 90690\n",
      "Contextual Dataset ready.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Define Context Features ---\n",
    "# Based on your previous steps, you used 3 binary features.\n",
    "NUM_CONTEXT_FEATURES = 3 \n",
    "\n",
    "FINAL_CONTEXTUAL_FEATURES = [\n",
    "    'under_pressure', \n",
    "    'counterpress', \n",
    "    'dribble_nutmeg'\n",
    "]\n",
    "\n",
    "nn_dataset = data.prepare_nn_dataset(df_with_targets, nn_final, target_cols=['nn_target', 'goal_flag'], context_cols = True, keep_context_ids = True )\n",
    "\n",
    "target_map = {\"Keep Possession\": 0, \"Lose Possession\": 1, \"Shot\": 2}\n",
    "\n",
    "# Apply mapping\n",
    "nn_dataset['nn_target_int'] = nn_dataset['nn_target'].map(target_map)\n",
    "\n",
    "# Ensure the context DataFrame is prepared (imputed NaN with 0.0)\n",
    "context_df = nn_dataset[FINAL_CONTEXTUAL_FEATURES].copy().fillna(0.0) \n",
    "\n",
    "# Re-check the number of features just to be safe\n",
    "print(f\"Number of Contextual Features being used: {context_df.shape[1]}\") \n",
    "print(f\"Contextual Features Head:\\n{context_df.head()}\")\n",
    "\n",
    "# --- 2. Define Inputs and Targets (Same as before) ---\n",
    "#layer_columns = [\"ball_layer\", \"teammates_layer\", \"opponents_layer\"]\n",
    "#event_targets = nn_dataset['nn_target_int'].values\n",
    "#goal_flags = nn_dataset['goal_flag'].values.astype(np.float32) \n",
    "\n",
    "# --- 3. Create Contextual Dataset Instance ---\n",
    "# CRITICAL: PitchDatasetMultiTask must be updated to accept the 4th input\n",
    "train_dataset_context = ContextPitchDatasetMultiTask(\n",
    "    nn_layers_df=nn_dataset[layer_columns], \n",
    "    event_targets=event_targets, \n",
    "    goal_flags=goal_flags,\n",
    "    contextual_features_df=context_df # NEW ARGUMENT\n",
    ")\n",
    "\n",
    "print(f\"Contextual Dataset Size: {len(train_dataset_context)}\")\n",
    "print(\"Contextual Dataset ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "hairy-warner",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Context CNN Epoch 1:   0%|          | 25/9069 [00:00<00:37, 243.40it/s, event_loss=1.07, loss=1.07, shot_loss=0]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for Contextual CNN Baseline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Context CNN Epoch 1: 100%|██████████| 9069/9069 [00:37<00:00, 243.87it/s, event_loss=0.483, loss=20.9, shot_loss=4.09] \n",
      "Context CNN Epoch 2: 100%|██████████| 9069/9069 [00:36<00:00, 250.01it/s, event_loss=0.828, loss=0.828, shot_loss=0]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Contextual CNN Training complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming event_class_weights and goal_pos_weight are defined from previous cells\n",
    "\n",
    "print(\"Starting training for Contextual CNN Baseline...\")\n",
    "\n",
    "context_baseline_model = train_model_context(\n",
    "    dataset=train_dataset_context, \n",
    "    event_class_weights=class_weights_event, # Use your calculated weights\n",
    "    goal_pos_weight=goal_pos_weight,         # Use your calculated pos_weight\n",
    "    num_context_features=NUM_CONTEXT_FEATURES\n",
    ")\n",
    "\n",
    "print(\"\\nContextual CNN Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "armed-literature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Contextual CNN Model...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "zero-dimensional arrays cannot be concatenated",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-2a450097ae56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nEvaluating Contextual CNN Model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m metrics = evaluate_model_context(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext_baseline_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataset_context\u001b[0m \u001b[0;31m# Evaluate on the contextual dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/files/pixlball/src/evaluate.py\u001b[0m in \u001b[0;36mevaluate_model_context\u001b[0;34m(model, dataset, batch_size)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;31m# CONCATENATE PROBABILITY ARRAYS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0mall_event_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_event_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m     \u001b[0mall_goal_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_goal_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_calculate_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_event_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_event_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_goal_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_goal_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_event_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_goal_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: zero-dimensional arrays cannot be concatenated"
     ]
    }
   ],
   "source": [
    "# Assuming evaluate_model_context is imported and available\n",
    "\n",
    "print(\"\\nEvaluating Contextual CNN Model...\")\n",
    "\n",
    "metrics = evaluate_model_context(\n",
    "    model=context_baseline_model, \n",
    "    dataset=train_dataset_context # Evaluate on the contextual dataset\n",
    ")\n",
    "\n",
    "# You can now compare 'context_metrics' with your 'baseline_metrics'\n",
    "# print(context_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
