{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "entertaining-object",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import importlib\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- 1. Path Setup ---\n",
    "# Absolute path to repo root (adjust if necessary)\n",
    "repo_root = \"/files/pixlball\"\n",
    "if repo_root not in sys.path:\n",
    "    sys.path.insert(0, repo_root) \n",
    "\n",
    "# --- 2. Project Module Imports ---\n",
    "# Import all project modules using clean names\n",
    "import src.config as config\n",
    "import src.dataset as dataset\n",
    "import src.train as train\n",
    "import src.evaluate as evaluate\n",
    "import src.data as data\n",
    "import src.losses as losses\n",
    "import src.model as model\n",
    "import src.utils as utils\n",
    "\n",
    "# --- 3. Module Reloading (CRITICAL for Notebook Development) ---\n",
    "# Reload dependencies in order: Config/Utils -> Data/Losses/Model -> Train/Dataset/Evaluate\n",
    "importlib.reload(config)\n",
    "importlib.reload(utils)\n",
    "importlib.reload(data)\n",
    "importlib.reload(model)\n",
    "importlib.reload(losses) \n",
    "importlib.reload(dataset)\n",
    "importlib.reload(train)\n",
    "importlib.reload(evaluate)\n",
    "\n",
    "# --- 4. Direct Imports (For clean code in subsequent cells) ---\n",
    "# Import essential classes and functions needed for the pipeline steps\n",
    "\n",
    "# Configuration\n",
    "from src.config import DEVICE \n",
    "\n",
    "# Data/Dataset Classes\n",
    "from src.dataset import PitchDatasetMultiTask, TemporalPitchDataset, ContextPitchDatasetMultiTask, FusionPitchDataset\n",
    "\n",
    "# Training Functions\n",
    "from src.train import train_model_base, train_model_lstm, train_model_context, train_model_lstm_fused\n",
    "\n",
    "# Evaluation/Helpers\n",
    "from src.evaluate import evaluate_model_base, evaluate_model_lstm, evaluate_model_context, evaluate_model_lstm_fused\n",
    "from src.losses import get_model_criteria\n",
    "from src.model import TinyCNN_MultiTask, HybridCNN_LSTM, TinyCNN_LSTM_Fused\n",
    "from src.utils import get_sequence_lengths\n",
    "\n",
    "# --- Final Check ---\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "radical-velvet",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_events = pd.read_parquet(os.path.join(repo_root, \"data\", \"events_data.parquet\"), engine=\"fastparquet\")\n",
    "data_360 = pd.read_parquet(os.path.join(repo_root, \"data\", \"sb360_data.parquet\"), engine=\"fastparquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "shared-integral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1278 events.\n"
     ]
    }
   ],
   "source": [
    "admin_events = [\n",
    "        'Starting XI', 'Half Start', 'Half End', 'Player On', 'Player Off',\n",
    "        'Substitution', 'Tactical Shift', 'Referee Ball-Drop', 'Injury Stoppage',\n",
    "        'Bad Behaviour', 'Shield'\n",
    "    ]\n",
    "\n",
    "cleaned_df = data.drop_events(data_events, rows_to_drop=admin_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cutting-marshall",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts of each outcome nn_target\n",
      "Keep Possession    71251\n",
      "Lose Possession    28252\n",
      "Shot                4830\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Example usage\n",
    "# -----------------------------\n",
    "columns_to_drop = ['clearance_body_part',\n",
    "                   'clearance_head',\n",
    "                   'clearance_left_foot',\n",
    "                   'clearance_other',\n",
    "                   'clearance_right_foot',\n",
    "                   'shot_technique',\n",
    "                   'substitution_replacement_id',\n",
    "                   'substitution_replacement',\n",
    "                   'substitution_outcome',\n",
    "                   'shot_saved_off_target',\n",
    "                   'pass_miscommunication',\n",
    "                   'goalkeeper_shot_saved_off_target',\n",
    "                   'goalkeeper_punched_out',\n",
    "                   'shot_first_time',\n",
    "                   'shot_first_time',\n",
    "                   'shot_body_part',\n",
    "                   'related_events',\n",
    "                   'pass_shot_assist', \n",
    "                   'pass_straight', \n",
    "                   'pass_switch', \n",
    "                   'pass_technique', \n",
    "                   'pass_through_ball',\n",
    "                   'goalkeeper_body_part',\n",
    "                   'goalkeeper_end_location', \n",
    "                   'goalkeeper_outcome', \n",
    "                   'goalkeeper_position', \n",
    "                   'goalkeeper_technique', \n",
    "                   'goalkeeper_type', \n",
    "                   'goalkeeper_penalty_saved_to_post', \n",
    "                   'goalkeeper_shot_saved_to_post', \n",
    "                   'goalkeeper_lost_out', \n",
    "                   'goalkeeper_Clear', \n",
    "                   'goalkeeper_In Play Safe',\n",
    "                   'shot_key_pass_id',\n",
    "                   'shot_one_on_one',\n",
    "                   'shot_end_location',\n",
    "                   'shot_type',\n",
    "                   'pass_angle',\n",
    "                   'pass_body_part',\n",
    "                   'pass_type',\n",
    "                   'pass_length',\n",
    "                   'pass_outswinging',\n",
    "                   'pass_inswinging',\n",
    "                   'pass_cross', \n",
    "                   'pass_cut_back', \n",
    "                   'pass_deflected', \n",
    "                   'pass_goal_assist', \n",
    "                   'pass_recipient', \n",
    "                   'pass_recipient_id', \n",
    "                   'pass_assisted_shot_id', \n",
    "                   'pass_no_touch', \n",
    "                   'pass_end_location', \n",
    "                   'pass_aerial_won',\n",
    "                   'pass_height',\n",
    "                   'substitution_outcome_id',\n",
    "                   'tactics',\n",
    "                   'block_deflection',\n",
    "                   'dribble_no_touch',\n",
    "                   'shot_open_goal', \n",
    "                   'shot_saved_to_post',\n",
    "                   'shot_redirect', \n",
    "                   'shot_follows_dribble',\n",
    "                   'period',\n",
    "                   'injury_stoppage_in_chanin',\n",
    "                   'block_save_block',\n",
    "                   'ball recovery_offensive',\n",
    "\n",
    "\n",
    "                   ]\n",
    "cleaned_df = data.drop_columns(cleaned_df, columns_to_drop)\n",
    "\n",
    "# add lookahead outcome\n",
    "df_with_targets = data.assign_lookahead_outcomes(cleaned_df, lookahead=6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nearby-league",
   "metadata": {},
   "source": [
    "# Prepare 360 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "short-interview",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_360 = data.assign_grid_cells(data_360)\n",
    "nn_final = data.aggregate_nn_layers_vectorized(df_360)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fantastic-senator",
   "metadata": {},
   "source": [
    "# Finalize NN Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cognitive-atlantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_dataset = data.prepare_nn_dataset(df_with_targets, nn_final, target_cols=['nn_target', 'goal_flag'], context_cols = True, keep_context_ids = True ) # adjust cols depending on model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convinced-calendar",
   "metadata": {},
   "source": [
    "# Neural Network final Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "amazing-palmer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         nn_target  nn_target_int\n",
      "0  Keep Possession              0\n",
      "1  Keep Possession              0\n",
      "2  Keep Possession              0\n",
      "3  Keep Possession              0\n",
      "4  Keep Possession              0\n"
     ]
    }
   ],
   "source": [
    "context_cols = [\n",
    "    'under_pressure', \n",
    "    'counterpress', \n",
    "    'dribble_nutmeg'\n",
    "]\n",
    "\n",
    "# Impute NaN values with 0.0 (float)\n",
    "# This assumes NaN means the event was NOT under pressure, NOT a counterpress, etc.\n",
    "nn_dataset[context_cols] = nn_dataset[context_cols].fillna(0.0)\n",
    "\n",
    "\n",
    "target_map = {\"Keep Possession\": 0, \"Lose Possession\": 1, \"Shot\": 2}\n",
    "\n",
    "# Apply mapping\n",
    "nn_dataset['nn_target_int'] = nn_dataset['nn_target'].map(target_map)\n",
    "\n",
    "# Check\n",
    "print(nn_dataset[['nn_target', 'nn_target_int']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "better-pledge",
   "metadata": {},
   "source": [
    "# The LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "signed-square",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Goal Positive Weight (0/1 ratio): 168.51\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "layer_columns = [\"ball_layer\", \"teammates_layer\", \"opponents_layer\"]\n",
    "\n",
    "# Get your existing targets\n",
    "event_targets = nn_dataset['nn_target_int'].values\n",
    "goal_flags = nn_dataset['goal_flag'].values.astype(np.float32) # Ensure targets are float\n",
    "\n",
    "event_counts = Counter(event_targets)\n",
    "total_events = len(event_targets)\n",
    "\n",
    "# Using inverse frequency: total / count\n",
    "class_weights_event = torch.tensor(\n",
    "    [total_events / event_counts.get(c, 1) for c in range(len(event_counts))],\n",
    "    dtype=torch.float32\n",
    ").to(DEVICE)\n",
    "\n",
    "# B. Goal Positive Weight (Binary) - For BCEWithLogitsLoss\n",
    "goal_counts = Counter(goal_flags)\n",
    "# CRITICAL: pos_weight = (Number of Negative Samples) / (Number of Positive Samples)\n",
    "# Here: pos_weight = (Number of No Goals) / (Number of Goals)\n",
    "goal_pos_weight = torch.tensor(\n",
    "    goal_counts.get(0.0, 1) / goal_counts.get(1.0, 1),\n",
    "    dtype=torch.float32\n",
    ").to(DEVICE)\n",
    "\n",
    "print(f\"Goal Positive Weight (0/1 ratio): {goal_pos_weight.item():.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "encouraging-marshall",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing 5D Sequential Dataset for LSTM...\n",
      "         nn_target  nn_target_int\n",
      "0  Keep Possession              0\n",
      "1  Keep Possession              0\n",
      "2  Keep Possession              0\n",
      "3  Keep Possession              0\n",
      "4  Keep Possession              0\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Prepare Sequential Dataset\n",
    "# -----------------------------\n",
    "print(\"Preparing 5D Sequential Dataset for LSTM...\")\n",
    "\n",
    "nn_dataset = data.prepare_nn_dataset(df_with_targets, nn_final, target_cols=['nn_target', 'goal_flag', 'possession']) # adjust cols depending on model\n",
    "\n",
    "target_map = {\"Keep Possession\": 0, \"Lose Possession\": 1, \"Shot\": 2}\n",
    "\n",
    "# Apply mapping\n",
    "nn_dataset['nn_target_int'] = nn_dataset['nn_target'].map(target_map)\n",
    "\n",
    "# Check\n",
    "print(nn_dataset[['nn_target', 'nn_target_int']].head())\n",
    "\n",
    "windows = data.build_temporal_windows_with_mask(nn_dataset)\n",
    "\n",
    "# Assuming 'windows' variable holds the output of data.build_temporal_windows_with_mask()\n",
    "# Shape: (Num_Events, T, 4, H, W)\n",
    "temporal_dataset = TemporalPitchDataset(\n",
    "    windows=windows, \n",
    "    event_labels=event_targets, \n",
    "    goal_flags=goal_flags\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-evaluation",
   "metadata": {},
   "source": [
    "# LSTM CNN with Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cooperative-title",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence Length (T): 4\n",
      "Context Sequence Shape: (90690, 4, 3) (N, T, F)\n",
      "Fused Dataset Size: 90690\n"
     ]
    }
   ],
   "source": [
    "# Assuming you previously defined these:\n",
    "NUM_CONTEXT_FEATURES = 3 \n",
    "FINAL_CONTEXTUAL_FEATURES = ['under_pressure', 'counterpress', 'dribble_nutmeg']\n",
    "nn_dataset = data.prepare_nn_dataset(df_with_targets, nn_final, target_cols=['nn_target', 'goal_flag'], context_cols = True, keep_context_ids = True ) # adjust cols depending on model\n",
    "# 1. Prepare 1D Context Features (from the static context_df)\n",
    "# Assuming 'context_df' is defined and NaN-imputed in a previous cell\n",
    "context_df = nn_dataset[FINAL_CONTEXTUAL_FEATURES].copy().fillna(0.0)\n",
    "context_features_array = context_df.values # Shape: (N_events, 3)\n",
    "\n",
    "# 2. Determine the Sequence Length (T)\n",
    "# CRITICAL FIX: The list 'windows' contains 'N' items. The length of the first item is T.\n",
    "if isinstance(windows, list) and len(windows) > 0:\n",
    "    # Use array conversion to safely get the shape of the first sequence item\n",
    "    T = np.array(windows[0]).shape[0] \n",
    "else:\n",
    "    # If windows is already a NumPy array, use its shape\n",
    "    T = windows.shape[1] \n",
    "\n",
    "print(f\"Sequence Length (T): {T}\")\n",
    "\n",
    "# 3. Create the Context Sequence (T-frames for each event)\n",
    "# Repeat the static context features (N, 3) across the T dimension: (N, T, 3)\n",
    "# context_features_array is (N, 3). We insert a T-dimension and repeat the feature vector T times.\n",
    "context_sequence = np.repeat(context_features_array[:, np.newaxis, :], T, axis=1)\n",
    "\n",
    "print(f\"Context Sequence Shape: {context_sequence.shape} (N, T, F)\")\n",
    "\n",
    "# --- 4. Create Fused Dataset Instance (CRITICAL: Reload and Import) ---\n",
    "# ... rest of your import and dataset creation code ...\n",
    "\n",
    "fused_dataset = FusionPitchDataset(\n",
    "    windows=windows, # The FusionPitchDataset class will handle the list to array conversion internally\n",
    "    contextual_features=context_sequence, \n",
    "    event_labels=event_targets, \n",
    "    goal_flags=goal_flags\n",
    ")\n",
    "\n",
    "print(f\"Fused Dataset Size: {len(fused_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "revised-stewart",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fused LSTM Epoch 1:   0%|          | 13/9069 [00:00<01:14, 122.04it/s, event_loss=1.08, loss=4.21, shot_loss=0.626]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for TinyCNN_LSTM_Fused Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fused LSTM Epoch 1: 100%|██████████| 9069/9069 [01:03<00:00, 142.68it/s, event_loss=1.19, loss=18.9, shot_loss=3.53]\n",
      "Fused LSTM Epoch 2: 100%|██████████| 9069/9069 [01:13<00:00, 123.58it/s, event_loss=0.907, loss=0.907, shot_loss=0]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TinyCNN_LSTM_Fused Training complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming class_weights_event and goal_pos_weight are defined\n",
    "\n",
    "print(\"Starting training for TinyCNN_LSTM_Fused Model...\")\n",
    "\n",
    "fused_lstm_model = train_model_lstm_fused(\n",
    "    dataset=fused_dataset, \n",
    "    event_class_weights=class_weights_event,\n",
    "    goal_pos_weight=goal_pos_weight,\n",
    "    num_context_features=NUM_CONTEXT_FEATURES # Pass the number of 1D features\n",
    ")\n",
    "\n",
    "print(\"\\nTinyCNN_LSTM_Fused Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "radio-pension",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Fused LSTM Model on training data...\n",
      "Event Accuracy: 0.6178409968022935\n",
      "Event Balanced Accuracy: 0.3699842548543841\n",
      "Event Confusion Matrix:\n",
      " [[47874 14785     0]\n",
      " [15426  8158     0]\n",
      " [ 3170  1277     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.76      0.74     62659\n",
      "           1       0.34      0.35      0.34     23584\n",
      "           2       0.00      0.00      0.00      4447\n",
      "\n",
      "    accuracy                           0.62     90690\n",
      "   macro avg       0.35      0.37      0.36     90690\n",
      "weighted avg       0.59      0.62      0.60     90690\n",
      "\n",
      "Goal Accuracy: 0.12030582415111311\n",
      "Goal Balanced Accuracy: 0.5\n",
      "Goal Confusion Matrix:\n",
      " [[   0 3912]\n",
      " [   0  535]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00      3912\n",
      "         1.0       0.12      1.00      0.21       535\n",
      "\n",
      "    accuracy                           0.12      4447\n",
      "   macro avg       0.06      0.50      0.11      4447\n",
      "weighted avg       0.01      0.12      0.03      4447\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEvaluating Fused LSTM Model on training data...\")\n",
    "\n",
    "fused_metrics = evaluate_model_lstm_fused(\n",
    "    model=fused_lstm_model, \n",
    "    dataset=fused_dataset\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
