{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "better-tournament",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using repo_root: c:\\Users\\jonas\\Desktop\\repos\\pixlball\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import importlib\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- 1. Path Setup ---\n",
    "# Try to locate the repository root by searching upward for a 'src' directory (or .git)\n",
    "def find_repo_root(start_path=None, marker_dirs=('src', '.git')):\n",
    "    p = os.path.abspath(start_path or os.getcwd())\n",
    "    while True:\n",
    "        if any(os.path.isdir(os.path.join(p, m)) for m in marker_dirs):\n",
    "            return p\n",
    "        parent = os.path.dirname(p)\n",
    "        if parent == p:\n",
    "            return None\n",
    "        p = parent\n",
    "\n",
    "repo_root = find_repo_root()\n",
    "# Fallback to previous hardcoded path working on nuvolos\n",
    "if repo_root is None:\n",
    "    repo_root = \"/files/pixlball\"\n",
    "\n",
    "if repo_root not in sys.path:\n",
    "    sys.path.insert(0, repo_root)\n",
    "print(f\"Using repo_root: {repo_root}\")\n",
    "\n",
    "# --- 2. Project Module Imports ---\n",
    "# Import all project modules using clean names\n",
    "import src.config as config\n",
    "import src.dataset as dataset\n",
    "import src.train as train\n",
    "import src.evaluate as evaluate\n",
    "import src.data as data\n",
    "import src.losses as losses\n",
    "import src.model as model\n",
    "import src.utils as utils\n",
    "\n",
    "# --- 3. Module Reloading (CRITICAL for Notebook Development) ---\n",
    "# Reload dependencies in order: Config/Utils -> Data/Losses/Model -> Train/Dataset/Evaluate\n",
    "importlib.reload(config)\n",
    "importlib.reload(utils)\n",
    "importlib.reload(data)\n",
    "importlib.reload(model)\n",
    "importlib.reload(losses) \n",
    "importlib.reload(dataset)\n",
    "importlib.reload(train)\n",
    "importlib.reload(evaluate)\n",
    "\n",
    "# --- 4. Direct Imports (For clean code in subsequent cells) ---\n",
    "# Import essential classes and functions needed for the pipeline steps\n",
    "\n",
    "# Configuration\n",
    "from src.config import DEVICE \n",
    "\n",
    "# Data/Dataset Classes\n",
    "from src.dataset import PitchDatasetMultiTask, ContextPitchDatasetMultiTask\n",
    "\n",
    "# Training Functions\n",
    "from src.train import train_model_base_threat, train_model_context_threat\n",
    "\n",
    "# Evaluation/Helpers\n",
    "from src.evaluate import evaluate_model_base_threat, evaluate_model_context_threat\n",
    "from src.losses import get_model_criteria, FocalLossThreat\n",
    "from src.model import TinyCNN_MultiTask_Threat\n",
    "from src.utils import get_sequence_lengths\n",
    "\n",
    "# --- Final Check ---\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "funny-municipality",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_events = pd.read_parquet(os.path.join(repo_root, \"data\", \"events_data.parquet\"), engine=\"fastparquet\")\n",
    "data_360 = pd.read_parquet(os.path.join(repo_root, \"data\", \"sb360_data.parquet\"), engine=\"fastparquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da53614b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2462 events.\n",
      "counts of each outcome nn_target\n",
      "Keep Possession    70920\n",
      "Lose Possession    27465\n",
      "Shot                4764\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_with_targets = data.event_data_loader(data_events)\n",
    "df_with_targets = data.add_ball_trajectory_features(df_with_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-taxation",
   "metadata": {},
   "source": [
    "# Prepare 360 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fallen-michael",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_360 = data.assign_grid_cells(data_360)\n",
    "nn_final = data.aggregate_nn_layers_vectorized(df_360)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-highlight",
   "metadata": {},
   "source": [
    "# Finalize NN Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "vocational-metadata",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_dataset = data.prepare_nn_dataset(df_with_targets, nn_final, target_cols=['nn_target', 'goal_flag'], context_cols = True, keep_context_ids = True ) # adjust cols depending on model\n",
    "nn_dataset = data.add_context_cols(nn_dataset)\n",
    "nn_dataset = data.add_target_as_int(nn_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-gender",
   "metadata": {},
   "source": [
    "# Neural Network final Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-carnival",
   "metadata": {},
   "source": [
    "# The Goal Multi Task CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "alleged-yahoo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goal Positive Weight (0/1 ratio): 5.00\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------\n",
    "# 1. Define input columns & targets\n",
    "# ------------------------------------\n",
    "# This assumes nn_dataset is already loaded and processed in previous cells.\n",
    "layer_columns = [\"ball_layer\", \"teammates_layer\", \"opponents_layer\"]\n",
    "class_weights_event, goal_pos_weight = utils.get_multitask_loss_weights(nn_dataset, DEVICE)\n",
    "\n",
    "print(f\"Goal Positive Weight (0/1 ratio): {goal_pos_weight.item():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handed-talent",
   "metadata": {},
   "source": [
    "# Preparing the Context CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "considerable-wednesday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 72117\n",
      "Total validation samples: 18030\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Define your split parameters\n",
    "VALIDATION_SIZE = 0.20\n",
    "RANDOM_SEED = 42\n",
    "layer_columns = [\"ball_layer\", \"teammates_layer\", \"opponents_layer\"]\n",
    "\n",
    "# 2. Split the entire DataFrame first\n",
    "# This keeps features, event targets, and goal flags bundled together\n",
    "train_df, val_df = train_test_split(\n",
    "    nn_dataset, \n",
    "    test_size=VALIDATION_SIZE, \n",
    "    random_state=RANDOM_SEED, \n",
    "    stratify=nn_dataset['nn_target_int']\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 3. Extract the arrays and Instantiate the Datasets (FIXED)\n",
    "# -------------------------------------------------------------\n",
    "context_features = ['under_pressure', 'counterpress', 'dribble_nutmeg']\n",
    "\n",
    "# Training Dataset extraction - Pass only the values in the correct order\n",
    "train_dataset_context = ContextPitchDatasetMultiTask(\n",
    "    train_df[layer_columns],             # This maps to the 1st argument (features)\n",
    "    train_df['nn_target_int'].values,    # This maps to the 2nd argument (events)\n",
    "    train_df['goal_flag'].values,\n",
    "    train_df[context_features]        # This maps to the 3rd argument (goals)\n",
    ")\n",
    "\n",
    "# Validation Dataset extraction\n",
    "validation_dataset_context = ContextPitchDatasetMultiTask(\n",
    "    val_df[layer_columns], \n",
    "    val_df['nn_target_int'].values, \n",
    "    val_df['goal_flag'].values,\n",
    "    val_df[context_features]  \n",
    ")\n",
    "\n",
    "print(f\"Total training samples: {len(train_dataset_context)}\")\n",
    "print(f\"Total validation samples: {len(validation_dataset_context)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "revised-president",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for Contextual CNN Baseline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Context CNN Epoch 1: 100%|██████████| 2254/2254 [00:38<00:00, 57.89it/s, ev_loss=1.5562, loss=2.3343, sh_loss=0.5187] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Contextual CNN Training complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming event_class_weights and goal_pos_weight are defined from previous cells\n",
    "NUM_CONTEXT_FEATURES = 3 \n",
    "\n",
    "print(\"Starting training for Contextual CNN Baseline...\")\n",
    "\n",
    "# Modified the Function in Loss to take correct loss function -> needs to be changed for baseline model again\n",
    "\n",
    "context_baseline_model = train_model_context_threat(\n",
    "    dataset=train_dataset_context, \n",
    "    event_class_weights=class_weights_event, # Use your calculated weights\n",
    "    goal_pos_weight=goal_pos_weight,         # Use your calculated pos_weight\n",
    "    num_context_features=NUM_CONTEXT_FEATURES\n",
    ")\n",
    "\n",
    "print(\"\\nContextual CNN Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "whole-player",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Contextual CNN Model...\n",
      "\n",
      "--- Event Outcome Metrics ---\n",
      "Event Accuracy: 0.2410981697171381\n",
      "Event Balanced Accuracy: 0.5289652051646935\n",
      "Event Confusion Matrix:\n",
      " [[    0 10378  2110]\n",
      " [    0  3633  1024]\n",
      " [    0   171   714]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00     12488\n",
      "           1       0.26      0.78      0.39      4657\n",
      "           2       0.19      0.81      0.30       885\n",
      "\n",
      "    accuracy                           0.24     18030\n",
      "   macro avg       0.15      0.53      0.23     18030\n",
      "weighted avg       0.08      0.24      0.11     18030\n",
      "\n",
      "\n",
      "--- Goal Prediction (xG) Metrics ---\n",
      "Goal Accuracy: 0.8610169491525423\n",
      "Goal Balanced Accuracy: 0.5048546471640148\n",
      "Goal AUC-ROC Score: 0.6111811637397404\n",
      "Goal Confusion Matrix:\n",
      " [[759  13]\n",
      " [110   3]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.98      0.93       772\n",
      "         1.0       0.19      0.03      0.05       113\n",
      "\n",
      "    accuracy                           0.86       885\n",
      "   macro avg       0.53      0.50      0.49       885\n",
      "weighted avg       0.79      0.86      0.81       885\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonas\\miniconda3\\envs\\standard\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\jonas\\miniconda3\\envs\\standard\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\jonas\\miniconda3\\envs\\standard\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "# Assuming evaluate_model_context is imported and available\n",
    "\n",
    "print(\"\\nEvaluating Contextual CNN Model...\")\n",
    "\n",
    "metrics = evaluate_model_context_threat(\n",
    "    model=context_baseline_model, \n",
    "    dataset=validation_dataset_context # Evaluate on the contextual dataset\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "emerging-channels",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Keep) | P(Lose) | P(Shot)\n",
      "-------------------------------\n",
      "[[0.33783638 0.52474153 0.1374221 ]\n",
      " [0.3605819  0.5405849  0.09883313]\n",
      " [0.25526795 0.42775807 0.31697395]\n",
      " [0.3516903  0.5296582  0.11865151]\n",
      " [0.12461421 0.22598945 0.64939636]]\n",
      "\n",
      "Average Predicted P(Shot) across all events: 0.2479\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Assuming metrics contains the result from evaluate_model_context_threat\n",
    "\n",
    "event_probs = metrics['event_probs']\n",
    "\n",
    "print(\"P(Keep) | P(Lose) | P(Shot)\")\n",
    "print(\"-------------------------------\")\n",
    "print(event_probs[:5])\n",
    "\n",
    "# You can look at the average predicted probability for the Shot class across all events:\n",
    "avg_p_shot = np.mean(event_probs[:, 2])\n",
    "print(f\"\\nAverage Predicted P(Shot) across all events: {avg_p_shot:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "polyphonic-reminder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Goal Prediction Probabilities (xG) Analysis ---\n",
      "Number of Shots Evaluated: 885\n",
      "\n",
      "Total Predicted xG: 377.58\n",
      "Total True Goals Scored: 113.00\n",
      "Average Predicted xG per Shot: 0.4266\n",
      "\n",
      "-- Calibration Check --\n",
      "Average xG for True Goals (should be high): 0.4397\n",
      "Average xG for Missed Shots (should be low): 0.4247\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Assuming metrics contains the result from evaluate_model_context_threat\n",
    "\n",
    "print(\"--- Goal Prediction Probabilities (xG) Analysis ---\")\n",
    "\n",
    "goal_probs = metrics['goal_probs']\n",
    "goal_labels = metrics['goal_labels'] # Actual outcome (0=No Goal, 1=Goal)\n",
    "\n",
    "print(f\"Number of Shots Evaluated: {len(goal_probs)}\")\n",
    "\n",
    "# 1. Total xG vs. Actual Goals\n",
    "total_predicted_xg = np.sum(goal_probs)\n",
    "total_true_goals = np.sum(goal_labels)\n",
    "avg_xg_per_shot = np.mean(goal_probs)\n",
    "\n",
    "print(f\"\\nTotal Predicted xG: {total_predicted_xg:.2f}\")\n",
    "print(f\"Total True Goals Scored: {total_true_goals:.2f}\")\n",
    "print(f\"Average Predicted xG per Shot: {avg_xg_per_shot:.4f}\")\n",
    "\n",
    "# 2. Calibration Check (Optional but helpful)\n",
    "# Compare the average predicted xG for shots that were goals vs. shots that were misses.\n",
    "\n",
    "# Create a DataFrame for easy slicing\n",
    "xg_df = pd.DataFrame({'xg': goal_probs, 'goal': goal_labels})\n",
    "\n",
    "avg_xg_goal = xg_df[xg_df['goal'] == 1]['xg'].mean()\n",
    "avg_xg_miss = xg_df[xg_df['goal'] == 0]['xg'].mean()\n",
    "\n",
    "print(\"\\n-- Calibration Check --\")\n",
    "print(f\"Average xG for True Goals (should be high): {avg_xg_goal:.4f}\")\n",
    "print(f\"Average xG for Missed Shots (should be low): {avg_xg_miss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b21e04a",
   "metadata": {},
   "source": [
    "# The Contextual Temporal Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8581874",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
